"""Evidence-based QA Pipeline.

This module provides the main entry point for evidence-based question answering.
It integrates all RP5-RP10 components into a single, easy-to-use function.

Usage:
    from jarvis_core.evidence_qa import run_evidence_qa

    answer = run_evidence_qa(
        query="What is CD73?",
        inputs=["paper.pdf", "https://example.com/article"],
    )
"""
from __future__ import annotations

import logging
import re
import uuid
from pathlib import Path
from typing import Literal

from .agents import AgentResult, BaseAgent, Citation
from .evidence import EvidenceStore
from .executor import ExecutionEngine
from .llm import LLMClient
from .planner import Planner
from .sources import ChunkResult, ExecutionContext, SourceDocument, ingest
from .task import Task, TaskCategory

logger = logging.getLogger("jarvis_core.evidence_qa")


# Evidence QA prompt template with claim-level citation
EVIDENCE_QA_SYSTEM_PROMPT = """You are an evidence-based research assistant.

RULES (MUST FOLLOW):
1. Only use the evidence chunks provided below to answer the question
2. Structure your answer as a list of CLAIMS (numbered statements)
3. Each CLAIM must reference at least one chunk_id in brackets
4. If evidence is insufficient for a claim, do NOT include it
5. Never invent facts not in the evidence

OUTPUT FORMAT:
Return your response as numbered claims, each with [chunk_id] references:

1. [First claim text] [chunk_abc123]
2. [Second claim text] [chunk_def456, chunk_ghi789]
3. ...

After claims, provide overall status:
STATUS: success (if all claims supported) or partial (if some claims uncertain)

AVAILABLE EVIDENCE:
{evidence_chunks}

QUESTION: {query}
"""



class EvidenceQAAgent(BaseAgent):
    """Agent for evidence-based QA with citation requirements.

    This agent is designed to answer questions based solely on
    provided evidence chunks, with mandatory citation of sources.
    """

    name = "evidence_qa"

    def __init__(self, llm: LLMClient) -> None:
        self.llm = llm

    def run_single(
        self,
        task: Task,
        context: ExecutionContext | None = None,
    ) -> AgentResult:
        """Run evidence-based QA.

        Args:
            task: The task containing the query.
            context: ExecutionContext with available evidence chunks.

        Returns:
            AgentResult with answer, status, and citations.
        """
        query = task.user_goal or task.title

        # Get relevant chunks for the query
        if context is not None:
            relevant_chunks = context.get_relevant_chunks_preview(query, k=8)
        else:
            relevant_chunks = []

        # Format evidence for prompt
        if relevant_chunks:
            evidence_text = "\n".join(
                f"[{c['chunk_id']}] ({c['locator']}): {c['preview']}"
                for c in relevant_chunks
            )
        else:
            evidence_text = "(No evidence available)"

        # Build prompt
        prompt = EVIDENCE_QA_SYSTEM_PROMPT.format(
            evidence_chunks=evidence_text,
            query=query,
        )

        # Call LLM
        messages = [{"role": "user", "content": prompt}]
        response = self.llm.chat(messages)

        # Parse response (returns claims too)
        answer, status, chunk_ids, claims = self._parse_response(response, relevant_chunks)

        # Build citations from chunk_ids
        citations: list[Citation] = []
        for chunk_id in chunk_ids:
            # Find matching chunk info
            chunk_info = next(
                (c for c in relevant_chunks if c["chunk_id"] == chunk_id),
                None,
            )
            if chunk_info:
                citations.append(
                    Citation(
                        chunk_id=chunk_id,
                        source="evidence",
                        locator=chunk_info["locator"],
                        quote="",  # Quote will be regenerated by ExecutionEngine
                    )
                )

        return AgentResult(
            status=status,
            answer=answer,
            citations=citations,
            meta={"agent": self.name, "claims": claims.to_dict()},
        )

    def _parse_response(
        self,
        response: str,
        available_chunks: list[dict],
    ) -> tuple[str, str, list[str], ClaimSet]:
        """Parse LLM response to extract claims, answer, status, and chunk_ids.

        Args:
            response: Raw LLM response.
            available_chunks: List of available chunk previews.

        Returns:
            Tuple of (answer, status, chunk_ids, claims).
        """
        from .claim import Claim, ClaimSet

        claims = ClaimSet()
        chunk_ids: list[str] = []
        available_ids = {c["chunk_id"] for c in available_chunks}

        # Parse numbered claims from response
        lines = response.strip().split("\n")
        for line in lines:
            line = line.strip()
            # Skip empty lines and STATUS line
            if not line or line.lower().startswith("status:"):
                continue

            # Parse numbered claim (e.g., "1. Claim text [chunk_id]")
            if line and line[0].isdigit() and "." in line:
                # Extract claim text and chunk_ids
                text, refs = self._extract_claim_and_refs(line, available_ids)
                if text:
                    claim = Claim.create(text, refs)
                    claims.add(claim)
                    chunk_ids.extend(refs)

        # Generate answer from claims
        answer = claims.generate_answer() if claims.claims else response.strip()

        # Extract status
        status = "partial"
        if "status:" in response.lower():
            if "success" in response.lower().split("status:")[-1]:
                status = "success"

        # Deduplicate chunk_ids
        chunk_ids = list(set(chunk_ids))

        return answer, status, chunk_ids, claims

    def _extract_claim_and_refs(
        self,
        line: str,
        available_ids: set,
    ) -> tuple[str, list[str]]:
        """Extract claim text and chunk_id references from a line.

        Args:
            line: A numbered claim line.
            available_ids: Set of valid chunk_ids.

        Returns:
            Tuple of (claim_text, list_of_chunk_ids).
        """

        refs: list[str] = []

        # Find all [chunk_id] patterns
        for chunk_id in available_ids:
            if chunk_id in line:
                refs.append(chunk_id)

        # Remove chunk_id references to get clean text
        text = line
        # Remove number prefix
        text = re.sub(r"^\d+\.\s*", "", text)
        # Remove chunk references in brackets
        text = re.sub(r"\[[\w\-,\s]+\]", "", text)
        text = text.strip()

        return text, refs


def _detect_input_type(input_path: str) -> Literal["pdf", "url", "local"]:
    """Detect the type of input.

    Args:
        input_path: Path or URL string.

    Returns:
        "pdf", "url", or "local".
    """
    if input_path.startswith(("http://", "https://")):
        return "url"
    if input_path.lower().endswith(".pdf"):
        return "pdf"
    return "local"


def _ingest_input(
    input_path: str,
    store: EvidenceStore,
    context: ExecutionContext,
) -> list[ChunkResult]:
    """Ingest an input into EvidenceStore.

    Args:
        input_path: Path or URL to ingest.
        store: EvidenceStore to populate.
        context: ExecutionContext to register chunks in.

    Returns:
        List of ChunkResult from ingestion.
    """
    input_type = _detect_input_type(input_path)

    if input_type == "pdf":
        from .pdf_extractor import ingest_pdf

        return ingest_pdf(input_path, store, context=context)

    elif input_type == "url":
        from .web_fetcher import ingest_url

        return ingest_url(input_path, store, context=context)

    else:
        # Local text file
        path = Path(input_path)
        if path.exists():
            text = path.read_text(encoding="utf-8")
        else:
            text = input_path  # Treat as raw text

        doc = SourceDocument(
            source="local",
            locator_base=f"local:{input_path}",
            text=text,
        )
        results = ingest(doc, store)
        context.add_chunks(results)
        return results


def run_evidence_qa(
    *,
    query: str,
    inputs: list[str],
    category: str = "generic",
    llm: LLMClient | None = None,
) -> str:
    """Run evidence-based QA pipeline (backward compatible).

    This is the main entry point for evidence-based question answering.
    Returns the answer as a string for backward compatibility.

    For structured result with citations/status, use run_evidence_qa_result().

    Args:
        query: The question to answer.
        inputs: List of input paths/URLs to use as evidence.
        category: Task category (default: "generic").
        llm: Optional LLMClient (creates default if not provided).

    Returns:
        The final answer string.

    Example:
        >>> answer = run_evidence_qa(
        ...     query="What is CD73?",
        ...     inputs=["paper.pdf"],
        ... )
    """
    result = run_evidence_qa_result(
        query=query,
        inputs=inputs,
        category=category,
        llm=llm,
    )
    return result.answer


def run_evidence_qa_result(
    *,
    query: str,
    inputs: list[str],
    category: str = "generic",
    llm: LLMClient | None = None,
) -> EvidenceQAResult:
    """Run evidence-based QA pipeline with structured result.

    This returns a structured result with answer, status, citations,
    and metadata for audit, reproduction, and external tool integration.

    Args:
        query: The question to answer.
        inputs: List of input paths/URLs to use as evidence.
        category: Task category (default: "generic").
        llm: Optional LLMClient (creates default if not provided).

    Returns:
        EvidenceQAResult with full structured data.

    Example:
        >>> result = run_evidence_qa_result(
        ...     query="What is CD73?",
        ...     inputs=["paper.pdf"],
        ... )
        >>> print(result.status, result.answer)
    """
    from .result import EvidenceQAResult

    # Create LLM client if not provided
    if llm is None:
        llm = LLMClient(model="gemini-2.0-flash")

    # Create EvidenceStore and ExecutionContext
    store = EvidenceStore()
    context = ExecutionContext(evidence_store=store)

    # Ingest all inputs
    for input_path in inputs:
        try:
            _ingest_input(input_path, store, context)
            logger.info("Ingested: %s", input_path)
        except Exception as e:
            logger.warning("Failed to ingest %s: %s", input_path, e)

    # Validate category
    try:
        task_category = TaskCategory(category)
    except ValueError:
        task_category = TaskCategory.GENERIC

    # Create task
    task = Task(
        task_id=str(uuid.uuid4()),
        title=query,
        category=task_category,
        user_goal=query,
        inputs={"query": query, "sources": inputs},
    )

    # Create agent and router
    agent = EvidenceQAAgent(llm)

    # Track the agent result for structured output
    agent_result_holder: list[AgentResult] = []

    class EvidenceQARouter:
        """Simple router that always uses EvidenceQAAgent."""

        def __init__(self, agent: EvidenceQAAgent, context: ExecutionContext):
            self.agent = agent
            self.context = context

        def run(self, task: Task) -> AgentResult:
            result = self.agent.run_single(task, context=self.context)
            agent_result_holder.append(result)
            return result

    router = EvidenceQARouter(agent, context)

    # Create planner
    planner = Planner()

    # Create and run ExecutionEngine
    engine = ExecutionEngine(
        planner=planner,
        router=router,
        evidence_store=store,
    )

    # Run and get answer
    answer = engine.run_and_get_answer(task)

    # Get the final status from the executed task
    # Note: ExecutionEngine may have modified the status
    final_status = "partial"  # Default
    citations: list[Citation] = []

    if agent_result_holder:
        agent_result = agent_result_holder[-1]
        citations = agent_result.citations or []

    # Build structured result
    return EvidenceQAResult(
        answer=answer,
        status=final_status,
        citations=citations,
        inputs=inputs,
        query=query,
        meta={
            "category": category,
            "llm_provider": getattr(llm, "provider", "unknown") if llm else "unknown",
        },
    )


# Export the EvidenceStore for bundle export
def get_evidence_store_for_bundle(
    inputs: list[str],
) -> EvidenceStore:
    """Create and populate an EvidenceStore from inputs.

    This is a utility for bundle export when you need
    access to the store separately.

    Args:
        inputs: List of input paths/URLs.

    Returns:
        Populated EvidenceStore.
    """
    store = EvidenceStore()
    context = ExecutionContext(evidence_store=store)

    for input_path in inputs:
        try:
            _ingest_input(input_path, store, context)
        except Exception as e:
            logger.warning("Failed to ingest %s: %s", input_path, e)

    return store

